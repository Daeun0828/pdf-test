# search.py - ê°œì„ ëœ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
import sys, json, argparse, re
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from collections import defaultdict
import time

# ì„¤ì •
ARTIFACTS_DIR = Path("./artifacts")
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

class SemanticSearch:
    def __init__(self, artifacts_dir=ARTIFACTS_DIR, model_name=MODEL_NAME):
        self.artifacts_dir = Path(artifacts_dir)
        self.model_name = model_name
        self.model = None
        self.embeddings = None
        self.chunks = None
        self.doc_stats = None
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ê²€ì¦"""
        print(f"ğŸ“‚ {self.artifacts_dir}ì—ì„œ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
        required_files = ["embeddings.npy", "chunks.jsonl"]
        for file in required_files:
            file_path = self.artifacts_dir / file
            if not file_path.exists():
                print(f"âŒ {file_path}ê°€ ì—†ìŠµë‹ˆë‹¤. preprocess_embed.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                sys.exit(1)
        
        # ì„ë² ë”© ë¡œë“œ
        try:
            self.embeddings = np.load(self.artifacts_dir / "embeddings.npy", allow_pickle=False)
            if self.embeddings.size == 0 or self.embeddings.ndim != 2:
                raise ValueError("ì˜ëª»ëœ ì„ë² ë”© ì°¨ì›")
            print(f"  âœ“ ì„ë² ë”©: {self.embeddings.shape}")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)
        
        # ì²­í¬ ë°ì´í„° ë¡œë“œ
        try:
            self.chunks = []
            with open(self.artifacts_dir / "chunks.jsonl", "r", encoding="utf-8") as f:
                for line_no, line in enumerate(f, 1):
                    try:
                        chunk = json.loads(line.strip())
                        self.chunks.append(chunk)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  ë¼ì¸ {line_no} JSON ì˜¤ë¥˜: {e}")
            
            print(f"  âœ“ ì²­í¬: {len(self.chunks)}ê°œ")
            
            if len(self.chunks) != len(self.embeddings):
                print(f"âš ï¸  ì²­í¬ ê°œìˆ˜({len(self.chunks)})ì™€ ì„ë² ë”© ê°œìˆ˜({len(self.embeddings)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ì²­í¬ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)
        
        # ë¬¸ì„œ í†µê³„ ê³„ì‚°
        self._calculate_doc_stats()
        
        print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\n")
    
    def _calculate_doc_stats(self):
        """ë¬¸ì„œë³„ í†µê³„ ê³„ì‚°"""
        self.doc_stats = defaultdict(lambda: {"chunk_count": 0, "total_chars": 0})
        
        for chunk in self.chunks:
            doc_id = chunk["doc_id"]
            self.doc_stats[doc_id]["chunk_count"] += 1
            self.doc_stats[doc_id]["total_chars"] += len(chunk["text"])
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if self.model is None:
            print(f"ğŸ¤– {self.model_name} ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = SentenceTransformer(self.model_name)
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    def cosine_similarity(self, query_vec, embeddings):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ìµœì í™”)"""
        # L2 ì •ê·œí™”
        query_norm = query_vec / (np.linalg.norm(query_vec) + 1e-9)
        embeddings_norm = embeddings / (np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-9)
        
        # ë‚´ì ìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        return embeddings_norm @ query_norm
    
    def search(self, query, top_k=10, threshold=0.0, group_by_doc=False):
        """ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ìˆ˜í–‰"""
        start_time = time.time()
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.model.encode([query], convert_to_numpy=True)[0]
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = self.cosine_similarity(query_embedding, self.embeddings)
        
        # ê²°ê³¼ ì •ë ¬
        ranked_indices = similarities.argsort()[::-1]
        
        results = []
        shown_count = 0
        
        if group_by_doc:
            # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”í•´ì„œ ë³´ì—¬ì£¼ê¸°
            doc_results = defaultdict(list)
            
            for idx in ranked_indices:
                if similarities[idx] < threshold:
                    break
                
                chunk = self.chunks[idx]
                doc_id = chunk["doc_id"]
                
                doc_results[doc_id].append({
                    "chunk": chunk,
                    "score": similarities[idx],
                    "index": idx
                })
                
                if len(doc_results) >= top_k:
                    break
            
            # ë¬¸ì„œë³„ ìµœê³  ì ìˆ˜ë¡œ ì •ë ¬
            for doc_id, doc_chunks in sorted(doc_results.items(), 
                                           key=lambda x: max(c["score"] for c in x[1]), 
                                           reverse=True):
                results.append({
                    "doc_id": doc_id,
                    "chunks": sorted(doc_chunks, key=lambda x: x["score"], reverse=True),
                    "max_score": max(c["score"] for c in doc_chunks)
                })
        else:
            # ê°œë³„ ì²­í¬ë³„ë¡œ ë³´ì—¬ì£¼ê¸°
            for idx in ranked_indices:
                if similarities[idx] < threshold:
                    break
                
                chunk = self.chunks[idx]
                results.append({
                    "chunk": chunk,
                    "score": similarities[idx],
                    "index": idx
                })
                
                shown_count += 1
                if shown_count >= top_k:
                    break
        
        search_time = time.time() - start_time
        
        return {
            "results": results,
            "query": query,
            "total_time": search_time,
            "total_chunks": len(self.chunks),
            "shown_count": len(results)
        }
    
    def print_stats(self):
        """ë°ì´í„° í†µê³„ ì¶œë ¥"""
        if not self.doc_stats:
            return
            
        print("ğŸ“Š ë¬¸ì„œë³„ í†µê³„:")
        total_chunks = sum(stats["chunk_count"] for stats in self.doc_stats.values())
        total_chars = sum(stats["total_chars"] for stats in self.doc_stats.values())
        
        for doc_id, stats in sorted(self.doc_stats.items(), 
                                  key=lambda x: x[1]["chunk_count"], reverse=True):
            chunk_pct = stats["chunk_count"] / total_chunks * 100
            print(f"  ğŸ“„ {doc_id}")
            print(f"     ì²­í¬: {stats['chunk_count']:,}ê°œ ({chunk_pct:.1f}%)")
            print(f"     ë¬¸ì: {stats['total_chars']:,}ê°œ")
        
        print(f"\nì´ {len(self.doc_stats)}ê°œ ë¬¸ì„œ, {total_chunks:,}ê°œ ì²­í¬, {total_chars:,}ê°œ ë¬¸ì\n")

def pretty_preview(text, max_len=500):
    """í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ìì—°ìŠ¤ëŸ¬ìš´ ìë¥´ê¸°)"""
    text = text.replace("\n", " ").strip()
    
    if len(text) <= max_len:
        return text
    
    # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
    truncated = text[:max_len]
    
    # í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ë ì°¾ê¸°
    sentence_endings = ['. ', '! ', '? ', 'ë‹¤. ', 'ìš”. ', 'ë‹ˆë‹¤. ', 'ìŠµë‹ˆë‹¤. ']
    
    best_cut = -1
    for ending in sentence_endings:
        pos = truncated.rfind(ending)
        if pos > best_cut:
            best_cut = pos + len(ending)
    
    if best_cut > max_len * 0.7:  # 70% ì´ìƒ ìœ„ì¹˜ì—ì„œ ëë‚˜ë©´ ì‚¬ìš©
        return truncated[:best_cut].strip()
    
    # ë¬¸ì¥ ëì„ ëª» ì°¾ìœ¼ë©´ ê³µë°±ì—ì„œ ìë¥´ê¸°
    space_pos = truncated.rfind(' ')
    if space_pos > max_len * 0.8:
        return truncated[:space_pos] + "..."
    
    return truncated + "..."

def print_search_results(search_results, group_by_doc=False):
    """ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥"""
    results = search_results["results"]
    query = search_results["query"]
    total_time = search_results["total_time"]
    
    print(f"ğŸ” ì§ˆì˜: '{query}'")
    print(f"â±ï¸  ê²€ìƒ‰ ì‹œê°„: {total_time:.3f}ì´ˆ")
    print(f"ğŸ“ ê²°ê³¼: {len(results)}ê°œ\n")
    
    if not results:
        print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.\n")
        return
    
    if group_by_doc:
        # ë¬¸ì„œë³„ ê·¸ë£¹ ì¶œë ¥
        for i, doc_result in enumerate(results, 1):
            doc_id = doc_result["doc_id"]
            chunks = doc_result["chunks"][:3]  # ë¬¸ì„œë‹¹ ìµœëŒ€ 3ê°œ ì²­í¬ë§Œ
            max_score = doc_result["max_score"]
            
            print(f"ğŸ“„ [{i}] {doc_id} (ìµœê³ ì ìˆ˜: {max_score:.3f})")
            
            for j, chunk_data in enumerate(chunks):
                chunk = chunk_data["chunk"]
                score = chunk_data["score"]
                print(f"  â””â”€ ì²­í¬ {chunk['chunk_id']} (ì ìˆ˜: {score:.3f})")
                print(f"     {pretty_preview(chunk['text'], 400)}")
                if j < len(chunks) - 1:
                    print()
            print("\n" + "â”€"*80 + "\n")
    else:
        # ê°œë³„ ì²­í¬ ì¶œë ¥
        for i, result in enumerate(results, 1):
            chunk = result["chunk"]
            score = result["score"]
            
            print(f"ğŸ“„ [{i}] {chunk['doc_id']} (ì²­í¬ {chunk['chunk_id']}) - ì ìˆ˜: {score:.3f}")
            print(f"   {pretty_preview(chunk['text'])}")
            print()

def main():
    ap = argparse.ArgumentParser(description="ì˜ë¯¸ë¡ ì  ë¬¸ì„œ ê²€ìƒ‰ (ê°œì„  ë²„ì „)")
    ap.add_argument("query", nargs="?", help="ê²€ìƒ‰í•  ì§ˆì˜ (ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ)")
    ap.add_argument("--artifacts", default="./artifacts", help="artifacts í´ë” ê²½ë¡œ")
    ap.add_argument("--model", default=MODEL_NAME, help="SentenceTransformer ëª¨ë¸")
    ap.add_argument("--topk", type=int, default=10, help="ìƒìœ„ kê°œ ê²°ê³¼")
    ap.add_argument("--threshold", type=float, default=0.0, help="ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜")
    ap.add_argument("--group-by-doc", action="store_true", help="ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”")
    ap.add_argument("--stats", action="store_true", help="ë°ì´í„° í†µê³„ë§Œ ì¶œë ¥")
    args = ap.parse_args()
    
    # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
    search_engine = SemanticSearch(args.artifacts, args.model)
    search_engine.load_data()
    
    if args.stats:
        search_engine.print_stats()
        return
    
    search_engine.load_model()
    search_engine.print_stats()
    
    if args.query:
        # ë‹¨ì¼ ê²€ìƒ‰
        results = search_engine.search(
            args.query, 
            top_k=args.topk, 
            threshold=args.threshold,
            group_by_doc=args.group_by_doc
        )
        print_search_results(results, args.group_by_doc)
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        print("ğŸ” ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ (ì¢…ë£Œ: 'quit' ë˜ëŠ” Ctrl+C)")
        print("ğŸ’¡ ëª…ë ¹ì–´: stats (í†µê³„), clear (í™”ë©´ ì§€ìš°ê¸°)")
        print("=" * 60)
        
        try:
            while True:
                query = input("\nì§ˆì˜: ").strip()
                
                if not query or query.lower() in ["quit", "exit", "q"]:
                    break
                
                if query.lower() == "stats":
                    search_engine.print_stats()
                    continue
                
                if query.lower() == "clear":
                    import os
                    os.system('clear' if os.name == 'posix' else 'cls')
                    continue
                
                results = search_engine.search(
                    query, 
                    top_k=args.topk, 
                    threshold=args.threshold,
                    group_by_doc=args.group_by_doc
                )
                print_search_results(results, args.group_by_doc)
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()