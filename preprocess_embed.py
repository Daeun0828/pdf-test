# preprocess_embed.py - ìµœì í™” ë²„ì „
import os, sys, json, argparse, re, unicodedata
from pathlib import Path
import numpy as np
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import gc
from time import time
from typing import List, Dict, Tuple, Optional

# â”€â”€ kss ì˜ì¡´ì„± ì²´í¬ (ë¬¸ì¥ ë‹¨ìœ„ ì²­í¬ìš©)
try:
    import kss
except ImportError:
    print("[ì˜¤ë¥˜] kssê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € `pip install kss`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)

# â”€â”€ ì‹œë„ëŸ¬ìš´ MuPDF ê²½ê³  ìˆ¨ê¸°ê¸°
fitz.TOOLS.mupdf_display_errors(False)

# ===== ì „ì²˜ë¦¬ ìœ í‹¸ (ìµœì í™”) =====
@lru_cache(maxsize=10000)  # ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ ì •ê·œí™” ìºì‹œ
def normalize_unicode_cached(text: str) -> str:
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text)
    return text.replace("\r", "\n")

def normalize_unicode(text: str) -> str:
    return normalize_unicode_cached(text) if text else ""

# ì •ê·œì‹ ì»´íŒŒì¼ (ì„±ëŠ¥ í–¥ìƒ)
HYPHEN_PATTERN = re.compile(r"-\n(?=[A-Za-z0-9])")
SPACE_PATTERN = re.compile(r"[ \t]+")
NEWLINE_PATTERN = re.compile(r"\n{3,}")

def fix_hyphen_linebreaks(text: str) -> str:
    """ì¤„ ë í•˜ì´í”ˆìœ¼ë¡œ ëŠê¸´ ì˜ì–´ ë‹¨ì–´ ë³µì›"""
    return HYPHEN_PATTERN.sub("", text)

def collapse_spaces(text: str) -> str:
    """ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬"""
    text = SPACE_PATTERN.sub(" ", text)
    text = NEWLINE_PATTERN.sub("\n\n", text)
    return text.strip()

def remove_simple_headers_footers(pages: List[str]) -> List[str]:
    """í˜ì´ì§€ë§ˆë‹¤ ë°˜ë³µë˜ëŠ” í—¤ë”/í‘¸í„° ì œê±° (ìµœì í™”)"""
    if len(pages) < 2:  # í˜ì´ì§€ ìˆ˜ê°€ ì ìœ¼ë©´ ìŠ¤í‚µ
        return pages
    
    def get_first_last_lines(page_text):
        lines = page_text.splitlines()
        first = next((line.strip() for line in lines if line.strip()), "")
        last = next((line.strip() for line in reversed(lines) if line.strip()), "")
        return first, last

    # ì²«/ë§ˆì§€ë§‰ ì¤„ ì¶”ì¶œ
    first_last_pairs = [get_first_last_lines(p) for p in pages]
    firsts, lasts = zip(*first_last_pairs)
    
    # ê°€ì¥ ë¹ˆë²ˆí•œ ì²«/ë§ˆì§€ë§‰ ì¤„ ì°¾ê¸° (ìµœì†Œ 30% ì´ìƒ ë°˜ë³µë˜ëŠ” ê²ƒë§Œ)
    min_occurrences = max(2, len(pages) * 0.3)
    
    from collections import Counter
    first_counts = Counter(firsts)
    last_counts = Counter(lasts)
    
    common_first = next((text for text, count in first_counts.most_common() 
                        if count >= min_occurrences and len(text) > 5), "")
    common_last = next((text for text, count in last_counts.most_common() 
                       if count >= min_occurrences and len(text) > 5), "")

    # í—¤ë”/í‘¸í„° ì œê±°
    cleaned = []
    for page in pages:
        lines = page.splitlines()
        if lines and common_first and lines[0].strip() == common_first:
            lines = lines[1:]
        if lines and common_last and lines[-1].strip() == common_last:
            lines = lines[:-1]
        cleaned.append("\n".join(lines))
    
    return cleaned

def clean_text_per_doc(pages: List[str], strip_headers: bool = False) -> str:
    """ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    # ë¹ˆ í˜ì´ì§€ í•„í„°ë§
    pages = [normalize_unicode(page) for page in pages if page and page.strip()]
    
    if not pages:
        return ""
    
    if strip_headers and len(pages) > 1:
        pages = remove_simple_headers_footers(pages)
    
    text = "\n\n".join(pages)
    text = fix_hyphen_linebreaks(text)
    text = collapse_spaces(text)
    return text

# ===== ë¬¸ì¥ ë¶„ë¦¬/ì²­í¬ (ìµœì í™”) =====
@lru_cache(maxsize=1000)
def split_sentences_ko_cached(text_hash: int, text: str) -> Tuple[str, ...]:
    """ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼ ìºì‹œ"""
    sents = [s.strip() for s in kss.split_sentences(text) if s and len(s.strip()) >= 3]
    return tuple(sents)

def split_sentences_ko(text: str) -> List[str]:
    """í•œêµ­ì–´ ìµœì í™” ë¬¸ì¥ ê²½ê³„ ë¶„ë¦¬"""
    if not text or len(text) < 10:
        return []
    
    text_hash = hash(text)
    cached_result = split_sentences_ko_cached(text_hash, text)
    return list(cached_result)

def chunk_by_sentences(sentences: List[str], target_len: int = 800, overlap_sents: int = 2) -> List[str]:
    """ë¬¸ì¥ ê¸°ë°˜ ì²­í‚¹ (ìµœì í™”)"""
    if not sentences:
        return []
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence_len = len(sentence)
        
        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
        if current_length + sentence_len + 1 <= target_len or not current_chunk:
            current_chunk.append(sentence)
            current_length += sentence_len + 1
        else:
            # í˜„ì¬ ì²­í¬ ì™„ì„±
            chunks.append(" ".join(current_chunk))
            
            # ìƒˆ ì²­í¬ ì‹œì‘ (ê²¹ì¹¨ ë¬¸ì¥ í¬í•¨)
            overlap_start = max(0, len(current_chunk) - overlap_sents)
            current_chunk = current_chunk[overlap_start:] + [sentence]
            current_length = sum(len(s) + 1 for s in current_chunk)
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    return chunks

# ===== PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìµœì í™”) =====
def extract_pages_words(pdf_path: str, max_pages: Optional[int] = None, 
                       line_merge_tol: float = 2.0) -> List[str]:
    """ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì¶”ì¶œí•´ ì¤„ë³„ ì •ë ¬Â·ê³µë°± ë³µì›"""
    pages = []
    
    try:
        with fitz.open(pdf_path) as doc:
            page_range = range(min(len(doc), max_pages) if max_pages else len(doc))
            
            for page_num in page_range:
                page = doc[page_num]
                words = page.get_text("words")
                
                if not words:
                    pages.append("")
                    continue
                
                # ì¢Œí‘œ ê¸°ì¤€ ì •ë ¬ (yì¢Œí‘œ ìš°ì„ , ê·¸ ë‹¤ìŒ xì¢Œí‘œ)
                words.sort(key=lambda w: (round(w[1], 1), w[0]))
                
                # ì¤„ë³„ë¡œ ê·¸ë£¹í™”
                lines = []
                current_line = []
                current_y = None
                
                for word_data in words:
                    x0, y0, x1, y1, word = word_data[:5]
                    
                    if current_y is None or abs(y0 - current_y) < line_merge_tol:
                        current_line.append(word)
                        current_y = y0 if current_y is None else current_y
                    else:
                        if current_line:
                            lines.append(" ".join(current_line))
                        current_line = [word]
                        current_y = y0
                
                # ë§ˆì§€ë§‰ ì¤„ ì²˜ë¦¬
                if current_line:
                    lines.append(" ".join(current_line))
                
                pages.append("\n".join(lines))
                
    except Exception as e:
        raise Exception(f"PDF ì½ê¸° ì‹¤íŒ¨: {e}")
    
    return pages

def extract_pages_text(pdf_path: str, max_pages: Optional[int] = None) -> List[str]:
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    pages = []
    
    try:
        with fitz.open(pdf_path) as doc:
            page_range = range(min(len(doc), max_pages) if max_pages else len(doc))
            
            for page_num in page_range:
                page_text = doc[page_num].get_text() or ""
                pages.append(page_text)
                
    except Exception as e:
        raise Exception(f"PDF ì½ê¸° ì‹¤íŒ¨: {e}")
    
    return pages

# ===== ë³‘ë ¬ PDF ì²˜ë¦¬ =====
def process_single_pdf(pdf_path: str, args) -> Optional[Dict]:
    """ë‹¨ì¼ PDF ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    doc_id = os.path.basename(pdf_path)
    
    try:
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if args.extract_mode == "words":
            pages = extract_pages_words(pdf_path, max_pages=args.max_pages, 
                                      line_merge_tol=args.line_merge_tol)
            # words ê²°ê³¼ê°€ ë¹„ë©´ textë¡œ í´ë°±
            if not any(p.strip() for p in pages):
                pages = extract_pages_text(pdf_path, max_pages=args.max_pages)
        else:
            pages = extract_pages_text(pdf_path, max_pages=args.max_pages)
        
        # ì „ì²˜ë¦¬
        cleaned = clean_text_per_doc(pages, strip_headers=args.strip_headers)
        
        if len(cleaned) < args.min_doc_len:
            return {"error": f"í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìŒ({len(cleaned)} chars)", "doc_id": doc_id}
        
        # ë¬¸ì¥ ë¶„ë¦¬ ë° ì²­í‚¹
        sentences = split_sentences_ko(cleaned)
        chunks = chunk_by_sentences(sentences, target_len=args.chunk_size, 
                                  overlap_sents=args.overlap_sents)
        
        if not chunks:
            return {"error": "ì²­í¬ 0ê°œ", "doc_id": doc_id}
        
        return {
            "doc_id": doc_id,
            "path": pdf_path,
            "cleaned_text": cleaned,
            "chunks": chunks,
            "num_sentences": len(sentences),
            "success": True
        }
        
    except Exception as e:
        return {"error": str(e), "doc_id": doc_id}

# ===== ì…ë ¥ ëª©ë¡ =====
def load_pdf_list(list_file: Path, explicit_list: Optional[List[str]]) -> List[str]:
    """PDF ëª©ë¡ ë¡œë“œ"""
    if explicit_list:
        return [str(Path(p).resolve()) for p in explicit_list]
    
    if not list_file.exists():
        print(f"[ì˜¤ë¥˜] {list_file} ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € scan_pdfs.pyë¡œ ëª©ë¡ íŒŒì¼ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")
        sys.exit(1)
    
    paths = []
    with open(list_file, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                paths.append(line)
    
    if not paths:
        print(f"[ì˜¤ë¥˜] {list_file} ì•ˆì— ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    return paths

# ===== ë©”ì¸ =====
def main():
    start_time = time()
    
    ap = argparse.ArgumentParser(description="PDF ì „ì²˜ë¦¬ + ë¬¸ì¥ ì²­í¬ + ì„ë² ë”© (ìµœì í™” ë²„ì „)")
    ap.add_argument("--list-file", default="top5_pdfs.txt", help="PDF ì ˆëŒ€ê²½ë¡œ ëª©ë¡ íŒŒì¼")
    ap.add_argument("--pdf", nargs="*", help="ëª©ë¡ íŒŒì¼ ëŒ€ì‹  ì§ì ‘ PDF ì ˆëŒ€ê²½ë¡œ ì§€ì •")
    ap.add_argument("--out", default="artifacts", help="ê²°ê³¼ ì €ì¥ í´ë”")
    ap.add_argument("--model", default="sentence-transformers/all-MiniLM-L6-v2", help="SentenceTransformer ëª¨ë¸")
    ap.add_argument("--chunk-size", type=int, default=800, help="ì²­í¬ í¬ê¸°")
    ap.add_argument("--overlap-sents", type=int, default=2, help="ë¬¸ì¥ ê²¹ì¹¨ ê°œìˆ˜")
    ap.add_argument("--min-doc-len", type=int, default=500, help="ìµœì†Œ ë¬¸ì„œ ê¸¸ì´")
    ap.add_argument("--batch-size", type=int, default=64, help="ì„ë² ë”© ë°°ì¹˜ í¬ê¸°")
    ap.add_argument("--extract-mode", choices=["words", "text"], default="words", help="PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ì‹")
    ap.add_argument("--max-pages", type=int, default=None, help="ë¬¸ì„œë‹¹ ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜")
    ap.add_argument("--line-merge-tol", type=float, default=2.0, help="words ëª¨ë“œ ì¤„ ë³‘í•© tolerance")
    ap.add_argument("--strip-headers", action="store_true", help="ë°˜ë³µ í—¤ë”/í‘¸í„° ì œê±°")
    ap.add_argument("--workers", type=int, default=2, help="ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜")
    ap.add_argument("--save-texts", action="store_true", help="ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ íŒŒì¼ë„ ì €ì¥")
    args = ap.parse_args()

    OUT_DIR = Path(args.out)
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    
    if args.save_texts:
        (OUT_DIR / "texts").mkdir(exist_ok=True)

    pdf_list = load_pdf_list(Path(args.list_file), args.pdf)
    print(f"[ì •ë³´] ì²˜ë¦¬ ëŒ€ìƒ {len(pdf_list)}ê°œ")
    for i, p in enumerate(pdf_list, 1):
        print(f" {i}. {os.path.basename(p)}")

    # ëª¨ë¸ ë¡œë“œ
    print(f"\n[ëª¨ë¸] {args.model} ë¡œë”©ì¤‘...")
    model = SentenceTransformer(args.model)

    # ë³‘ë ¬ PDF ì²˜ë¦¬
    print(f"\n[ì²˜ë¦¬] {args.workers}ê°œ ìŠ¤ë ˆë“œë¡œ PDF ì²˜ë¦¬ì¤‘...")
    processed_results = []
    
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        future_to_pdf = {executor.submit(process_single_pdf, pdf, args): pdf for pdf in pdf_list}
        
        for i, future in enumerate(as_completed(future_to_pdf), 1):
            pdf_path = future_to_pdf[future]
            doc_name = os.path.basename(pdf_path)
            
            try:
                result = future.result()
                if result.get("success"):
                    processed_results.append(result)
                    print(f"  âœ“ [{i}/{len(pdf_list)}] {doc_name}: {len(result['chunks'])} chunks")
                else:
                    print(f"  âœ— [{i}/{len(pdf_list)}] {doc_name}: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            except Exception as e:
                print(f"  âœ— [{i}/{len(pdf_list)}] {doc_name}: ì²˜ë¦¬ ì‹¤íŒ¨ - {e}")

    if not processed_results:
        print("[ì˜¤ë¥˜] ì²˜ë¦¬ëœ PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    print(f"\nì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ PDF: {len(processed_results)}ê°œ")

    # ì„ë² ë”© ìƒì„±
    all_chunks = []
    all_records = []
    all_metas = []

    for result in processed_results:
        doc_id = result["doc_id"]
        chunks = result["chunks"]
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ (ì˜µì…˜)
        if args.save_texts:
            with open(OUT_DIR / "texts" / f"{doc_id}.txt", "w", encoding="utf-8") as f:
                f.write(result["cleaned_text"])
        
        # ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        for j, chunk in enumerate(chunks):
            all_chunks.append(chunk)
            all_records.append({
                "doc_id": doc_id,
                "chunk_id": j,
                "text": chunk,
                "source": result["path"]
            })
        
        all_metas.append({
            "doc_id": doc_id,
            "path": result["path"],
            "num_chunks": len(chunks),
            "num_sentences": result["num_sentences"],
            "num_chars": len(result["cleaned_text"])
        })

    # ì„ë² ë”© ìƒì„±
    print(f"\n[ì„ë² ë”©] ì´ {len(all_chunks)}ê°œ ì²­í¬ ì„ë² ë”© ìƒì„±ì¤‘...")
    embeddings = model.encode(all_chunks, convert_to_numpy=True, 
                            show_progress_bar=True, batch_size=args.batch_size)

    # ê²°ê³¼ ì €ì¥
    print("\n[ì €ì¥] ê²°ê³¼ íŒŒì¼ ìƒì„±ì¤‘...")
    
    # ì²­í¬ ë°ì´í„° ì €ì¥
    with open(OUT_DIR / "chunks.jsonl", "w", encoding="utf-8") as f:
        for record in all_records:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

    # ì„ë² ë”© ì €ì¥
    np.save(OUT_DIR / "embeddings.npy", embeddings.astype(np.float32))
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    with open(OUT_DIR / "meta.json", "w", encoding="utf-8") as f:
        json.dump(all_metas, f, ensure_ascii=False, indent=2)

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del embeddings, all_chunks
    gc.collect()

    elapsed = time() - start_time
    print(f"\nâœ… ì™„ë£Œ! (ì´ ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ)")
    print(f" - {OUT_DIR / 'chunks.jsonl'} ({len(all_records):,}ê°œ ì²­í¬)")
    print(f" - {OUT_DIR / 'embeddings.npy'} ({len(all_records):,}ê°œ ë²¡í„°)")
    print(f" - {OUT_DIR / 'meta.json'}")
    if args.save_texts:
        print(f" - {OUT_DIR / 'texts'}/*.txt")

    # ì²˜ë¦¬ í†µê³„
    total_chunks = sum(meta["num_chunks"] for meta in all_metas)
    total_chars = sum(meta["num_chars"] for meta in all_metas)
    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0
    
    print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
    print(f" - ë¬¸ì„œ: {len(all_metas)}ê°œ")
    print(f" - ì´ ì²­í¬: {total_chunks:,}ê°œ")
    print(f" - ì´ ë¬¸ì: {total_chars:,}ê°œ")
    print(f" - í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_size:.0f}ì")
    print(f" - ì²˜ë¦¬ ì†ë„: {total_chunks/elapsed:.1f} ì²­í¬/ì´ˆ")

if __name__ == "__main__":
    main()