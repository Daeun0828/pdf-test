# export_texts.py - ê°œì„ ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°
import argparse
import json
from pathlib import Path
import re
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import time

def slugify(name: str) -> str:
    """íŒŒì¼ëª…ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    # í™•ì¥ì ë¶„ë¦¬
    if '.' in name:
        stem = name[:name.rfind('.')]
        ext = name[name.rfind('.'):]
    else:
        stem, ext = name, ""
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°/ë³€í™˜
    safe_name = re.sub(r"[\\/:\*\?\"<>\|]", "_", stem)
    safe_name = re.sub(r"\s+", "_", safe_name)  # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ
    safe_name = re.sub(r"_+", "_", safe_name)   # ì—°ì† ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    safe_name = safe_name.strip("._")           # ì•ë’¤ íŠ¹ìˆ˜ë¬¸ì ì œê±°
    
    # ë„ˆë¬´ ê¸´ ì´ë¦„ ì¤„ì´ê¸°
    if len(safe_name) > 100:
        safe_name = safe_name[:100]
    
    return safe_name if safe_name else "document"

def load_chunks(chunks_path: Path) -> Dict[str, List[Tuple]]:
    """ì²­í¬ ë°ì´í„°ë¥¼ ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”"""
    by_doc = defaultdict(list)
    
    try:
        with open(chunks_path, "r", encoding="utf-8") as f:
            for line_no, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    obj = json.loads(line)
                    doc_id = obj.get("doc_id", f"unknown_{line_no}")
                    chunk_id = obj.get("chunk_id", 0)
                    text = obj.get("text", "")
                    source = obj.get("source")
                    
                    by_doc[doc_id].append((chunk_id, text, source))
                    
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  ë¼ì¸ {line_no} JSON ì˜¤ë¥˜: {e}")
                    continue
    
    except Exception as e:
        raise Exception(f"ì²­í¬ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    # ì²­í¬ ID ìˆœì„œë¡œ ì •ë ¬
    for doc_id in by_doc:
        by_doc[doc_id].sort(key=lambda x: x[0])
    
    return dict(by_doc)

def merge_chunks(chunks: List[Tuple], mode: str = "standard") -> str:
    """ì²­í¬ë“¤ì„ ë³‘í•© (ì—¬ëŸ¬ ëª¨ë“œ ì§€ì›)"""
    if not chunks:
        return ""
    
    texts = [text for _, text, _ in chunks]
    
    if mode == "standard":
        # ê¸°ë³¸: ë‘ ì¤„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
        return "\n\n".join(texts)
    
    elif mode == "compact":
        # ì••ì¶•: í•œ ì¤„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
        return "\n".join(texts)
    
    elif mode == "seamless":
        # ëŠê¹€ì—†ì´: ê³µë°± í•˜ë‚˜ë¡œ ì´ì–´ë¶™ì„
        return " ".join(text.strip() for text in texts)
    
    elif mode == "chapter":
        # ì±•í„°ë³„: ë²ˆí˜¸ ë§¤ê¸°ê¸°
        result = []
        for i, text in enumerate(texts, 1):
            result.append(f"## ì²­í¬ {i}\n\n{text}")
        return "\n\n".join(result)
    
    else:
        return "\n\n".join(texts)

def write_txt(out_dir: Path, doc_id: str, merged_text: str, stats: Dict) -> Path:
    """TXT íŒŒì¼ ì‘ì„±"""
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / f"{slugify(doc_id)}.txt"
    
    # ê°„ë‹¨í•œ í—¤ë” ì¶”ê°€
    header = f"ë¬¸ì„œ: {doc_id}\n"
    header += f"ì²­í¬ ìˆ˜: {stats['chunk_count']}ê°œ\n"
    header += f"ì´ ë¬¸ì: {stats['total_chars']:,}ì\n"
    header += f"ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
    header += "=" * 50 + "\n\n"
    
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(header + merged_text)
    
    return out_path

def write_md(out_dir: Path, doc_id: str, merged_text: str, source_path: Optional[str], 
            stats: Dict, mode: str = "standard") -> Path:
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì‘ì„±"""
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / f"{slugify(doc_id)}.md"
    
    # ë§ˆí¬ë‹¤ìš´ í—¤ë” ìƒì„±
    header = f"# {doc_id}\n\n"
    
    # ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
    header += "## ë¬¸ì„œ ì •ë³´\n\n"
    header += "| í•­ëª© | ê°’ |\n"
    header += "|------|----|\n"
    if source_path:
        header += f"| ì›ë³¸ ê²½ë¡œ | `{source_path}` |\n"
    header += f"| ì²­í¬ ìˆ˜ | {stats['chunk_count']}ê°œ |\n"
    header += f"| ì´ ë¬¸ì | {stats['total_chars']:,}ì |\n"
    header += f"| í‰ê·  ì²­í¬ í¬ê¸° | {stats['avg_chunk_size']:.0f}ì |\n"
    header += f"| ìƒì„± ì‹œê°„ | {time.strftime('%Y-%m-%d %H:%M:%S')} |\n\n"
    
    # ëª©ì°¨ ìƒì„± (chapter ëª¨ë“œì¼ ë•Œ)
    if mode == "chapter":
        header += "## ëª©ì°¨\n\n"
        chunk_count = stats['chunk_count']
        for i in range(1, chunk_count + 1):
            header += f"- [ì²­í¬ {i}](#ì²­í¬-{i})\n"
        header += "\n"
    
    header += "---\n\n## ë‚´ìš©\n\n"
    
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(header + merged_text)
    
    return out_path

def calculate_stats(chunks: List[Tuple]) -> Dict:
    """ì²­í¬ í†µê³„ ê³„ì‚°"""
    if not chunks:
        return {"chunk_count": 0, "total_chars": 0, "avg_chunk_size": 0}
    
    total_chars = sum(len(text) for _, text, _ in chunks)
    chunk_count = len(chunks)
    avg_chunk_size = total_chars / chunk_count if chunk_count > 0 else 0
    
    return {
        "chunk_count": chunk_count,
        "total_chars": total_chars,
        "avg_chunk_size": avg_chunk_size
    }

def print_summary(doc_stats: Dict[str, Dict]):
    """ì „ì²´ ìš”ì•½ ì¶œë ¥"""
    if not doc_stats:
        return
    
    print("\n" + "="*60)
    print("ğŸ“Š ì²˜ë¦¬ ìš”ì•½")
    print("="*60)
    
    total_docs = len(doc_stats)
    total_chunks = sum(stats["chunk_count"] for stats in doc_stats.values())
    total_chars = sum(stats["total_chars"] for stats in doc_stats.values())
    
    print(f"ì´ ë¬¸ì„œ: {total_docs}ê°œ")
    print(f"ì´ ì²­í¬: {total_chunks:,}ê°œ")
    print(f"ì´ ë¬¸ì: {total_chars:,}ê°œ")
    
    if total_docs > 0:
        avg_chunks_per_doc = total_chunks / total_docs
        avg_chars_per_doc = total_chars / total_docs
        print(f"ë¬¸ì„œë‹¹ í‰ê·  ì²­í¬: {avg_chunks_per_doc:.1f}ê°œ")
        print(f"ë¬¸ì„œë‹¹ í‰ê·  ë¬¸ì: {avg_chars_per_doc:,.0f}ì")
    
    print("\në¬¸ì„œë³„ ìƒì„¸:")
    for doc_id, stats in sorted(doc_stats.items(), 
                               key=lambda x: x[1]["total_chars"], reverse=True):
        print(f"  ğŸ“„ {doc_id}")
        print(f"     ì²­í¬: {stats['chunk_count']}ê°œ, ë¬¸ì: {stats['total_chars']:,}ì")

def main():
    parser = argparse.ArgumentParser(
        description="chunks.jsonlì„ ë¬¸ì„œë³„ë¡œ ë³‘í•©í•˜ì—¬ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± (ê°œì„  ë²„ì „)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ë³‘í•© ëª¨ë“œ:
  standard  - ê¸°ë³¸ ëª¨ë“œ (ì²­í¬ê°„ ë‘ ì¤„ ê³µë°±)
  compact   - ì••ì¶• ëª¨ë“œ (ì²­í¬ê°„ í•œ ì¤„ ê³µë°±) 
  seamless  - ì—°ì† ëª¨ë“œ (ê³µë°± í•˜ë‚˜ë¡œ ì´ì–´ë¶™ì„)
  chapter   - ì±•í„° ëª¨ë“œ (ë²ˆí˜¸ ë§¤ê¸°ê¸°)

ì‚¬ìš© ì˜ˆì‹œ:
  python export_texts.py
  python export_texts.py --mode compact --out-md markdown
  python export_texts.py --filter "ë°±ì‹ ,GMP" --stats-only
        """
    )
    
    parser.add_argument("--artifacts", default="artifacts", 
                       help="artifacts í´ë” ê²½ë¡œ (ê¸°ë³¸: artifacts)")
    parser.add_argument("--out-txt", default="artifacts/texts_merged", 
                       help="TXT ì¶œë ¥ í´ë”")
    parser.add_argument("--out-md", default=None, 
                       help="MD(ë§ˆí¬ë‹¤ìš´) ì¶œë ¥ í´ë” (ì„ íƒ)")
    parser.add_argument("--mode", choices=["standard", "compact", "seamless", "chapter"],
                       default="standard", help="ë³‘í•© ëª¨ë“œ")
    parser.add_argument("--filter", default=None,
                       help="íŠ¹ì • ë¬¸ì„œë§Œ ì²˜ë¦¬ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ)")
    parser.add_argument("--exclude", default=None,
                       help="ì œì™¸í•  ë¬¸ì„œ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ)")
    parser.add_argument("--min-chunks", type=int, default=1,
                       help="ìµœì†Œ ì²­í¬ ìˆ˜ (ê¸°ë³¸: 1)")
    parser.add_argument("--stats-only", action="store_true",
                       help="í†µê³„ë§Œ ì¶œë ¥í•˜ê³  íŒŒì¼ ìƒì„± ì•ˆ í•¨")
    
    args = parser.parse_args()

    # ì…ë ¥ íŒŒì¼ í™•ì¸
    art = Path(args.artifacts)
    chunks_path = art / "chunks.jsonl"
    if not chunks_path.exists():
        print(f"âŒ {chunks_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € preprocess_embed.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return 1

    print(f"ğŸ“‚ {chunks_path}ì—ì„œ ì²­í¬ ë°ì´í„° ë¡œë”© ì¤‘...")
    by_doc = load_chunks(chunks_path)
    
    if not by_doc:
        print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 1
    
    print(f"âœ… {len(by_doc)}ê°œ ë¬¸ì„œ ë°œê²¬")
    
    # í•„í„°ë§
    if args.filter:
        filter_keywords = [k.strip().lower() for k in args.filter.split(",")]
        filtered = {}
        for doc_id, chunks in by_doc.items():
            if any(keyword in doc_id.lower() for keyword in filter_keywords):
                filtered[doc_id] = chunks
        by_doc = filtered
        print(f"ğŸ” í•„í„° ì ìš©: {len(by_doc)}ê°œ ë¬¸ì„œ ì„ íƒ")
    
    if args.exclude:
        exclude_keywords = [k.strip().lower() for k in args.exclude.split(",")]
        filtered = {}
        for doc_id, chunks in by_doc.items():
            if not any(keyword in doc_id.lower() for keyword in exclude_keywords):
                filtered[doc_id] = chunks
        by_doc = filtered
        print(f"ğŸš« ì œì™¸ ì ìš©: {len(by_doc)}ê°œ ë¬¸ì„œ ë‚¨ìŒ")
    
    # ìµœì†Œ ì²­í¬ ìˆ˜ í•„í„°
    if args.min_chunks > 1:
        filtered = {doc_id: chunks for doc_id, chunks in by_doc.items() 
                   if len(chunks) >= args.min_chunks}
        by_doc = filtered
        print(f"ğŸ“Š ìµœì†Œ ì²­í¬ í•„í„°: {len(by_doc)}ê°œ ë¬¸ì„œ ë‚¨ìŒ")
    
    # í†µê³„ ìˆ˜ì§‘
    doc_stats = {}
    for doc_id, chunks in by_doc.items():
        doc_stats[doc_id] = calculate_stats(chunks)
    
    # í†µê³„ë§Œ ì¶œë ¥í•˜ê³  ì¢…ë£Œ
    if args.stats_only:
        print_summary(doc_stats)
        return 0
    
    # íŒŒì¼ ìƒì„±
    txt_dir = Path(args.out_txt)
    md_dir = Path(args.out_md) if args.out_md else None
    
    print(f"\nğŸ“ {args.mode} ëª¨ë“œë¡œ íŒŒì¼ ìƒì„± ì¤‘...")
    
    processed = 0
    for doc_id, chunks in by_doc.items():
        try:
            merged_text = merge_chunks(chunks, args.mode)
            stats = doc_stats[doc_id]
            
            # TXT íŒŒì¼ ìƒì„±
            txt_path = write_txt(txt_dir, doc_id, merged_text, stats)
            
            # MD íŒŒì¼ ìƒì„± (ì˜µì…˜)
            md_path = None
            if md_dir:
                source_path = chunks[0][2] if chunks and chunks[0][2] else None
                md_path = write_md(md_dir, doc_id, merged_text, source_path, stats, args.mode)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            print(f"  âœ… {doc_id}")
            print(f"      ì²­í¬: {stats['chunk_count']}ê°œ â†’ ë¬¸ì: {stats['total_chars']:,}ì")
            print(f"      ğŸ“ {txt_path.name}")
            if md_path:
                print(f"      ğŸ“ {md_path.name}")
            
            processed += 1
            
        except Exception as e:
            print(f"  âŒ {doc_id}: ì²˜ë¦¬ ì‹¤íŒ¨ - {e}")
    
    # ìµœì¢… ìš”ì•½
    print(f"\nâœ… ì™„ë£Œ: {processed}ê°œ ë¬¸ì„œ ì²˜ë¦¬")
    print(f"ğŸ“ TXT: {txt_dir.resolve()}")
    if md_dir:
        print(f"ğŸ“ MD:  {md_dir.resolve()}")
    
    print_summary(doc_stats)
    
    return 0

if __name__ == "__main__":
    exit(main())